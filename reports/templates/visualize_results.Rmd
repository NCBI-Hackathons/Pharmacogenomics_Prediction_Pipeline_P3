---
title: "P3: Visualization of Results"
date: "`r format(Sys.time(), '%d %B, %Y')`"
params:
    results_glob_str: ""
    output_dir: ""
output:
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
  pdf_document:
    dev: png
    toc: true
    latex_engine: xelatex
---

```{r load_libs, message=FALSE, warning=FALSE, include=FALSE}
library('DT')
library('dplyr')
library('knitr')
library('ggplot2')
library('gplots')
library('reshape2')
library('stringr')
library('SuperLearner')
library('RColorBrewer')
library('org.Hs.eg.db')
library('GO.db')
source('R/results_helper_functions.R')

options(stringsAsFactors=FALSE,
        knitr.duplicate.label='allow',
        digits=4)

# If rmarkdown.pandoc.to not specified (for example, when kniting
# piece-by-piece in Vim-R), have it default to 'latex' output.
if (is.null(opts_knit$get("rmarkdown.pandoc.to"))) {
    opts_knit$set(rmarkdown.pandoc.to='latex')
}

# For images containing unicode text, we will use cairo for PDF output
if (opts_knit$get("rmarkdown.pandoc.to") == 'latex') {
    opts_chunk$set(dev='cairo_pdf',
                   dev.args=list(cairo_pdf=list(family="DejaVu Sans")),
                   fig.width=1920/192,
                   fig.height=1920/192,
                   dpi=192)
} else {
    # HTML output
    opts_chunk$set(dev='png',
                   fig.width=1600/192,
                   fig.height=1600/192,
                   dpi=192, fig.retina=1)
}
```

Results
=======

### Load SuperLearner results

```{r message=FALSE}
run            <- parse_p3_results(Sys.glob(params$results_glob_str))
coefs          <- run$coefs
cv_risks       <- run$cv_risks
feature_importance <- run$feature_importance
```

### Summary of SuperLearner predictor performance

#### Absolute CV risk

```{r predictor_performance, results='asis'}
# CV risk averages across all drugs
cv_risks_long <- melt(cv_risks)
colnames(cv_risks_long) <- c('drug', 'learner', 'value')
cv_risks_long$learner <- as.character(cv_risks_long$learner)

cvrisk_averages <- cv_risks_long %>%
    group_by(learner) %>%
    summarise(average_risk=mean(value, na.rm=TRUE))

# Coefficient averages across all drugs
coefs_long <- melt(coefs)
colnames(coefs_long) <- c('drug', 'learner', 'value')
coefs_long$learner <- as.character(coefs_long$learner)

coef_averages <- coefs_long %>%
    group_by(learner) %>%
    summarise(average_coef=mean(value, na.rm=TRUE)) %>%
    arrange(desc(average_coef))

# For each algorithm, how many compounds have nonzero coeficients?
coef_nonzero <- coefs_long %>% 
    group_by(learner) %>% 
    summarise(ratio_nonzero=sum(value > 0) / length(value)) %>%
    arrange(desc(ratio_nonzero))

combined <- merge(cvrisk_averages, 
                  merge(coef_averages, coef_nonzero, by='learner'),
                  by='learner') %>%
    arrange(desc(ratio_nonzero))
xkable(combined)
```

#### Relative risk (~R^2)

##### R^2 for each Learner

```{r, results='asis', fig.cap='Distribution of relative CV risks', fig.width=1920/192, fig.height=1920/192}
rel_risks <- 1 - (cv_risks/cv_risks[,'mean'])
rel_risks <- rel_risks[,colnames(rel_risks) != 'mean']

# gbmOOB_* returned NA on rare occassions
summary_df <- as.data.frame(cbind(min=apply(rel_risks, 2, min, na.rm=TRUE),
                                  median=apply(rel_risks, 2, median, na.rm=TRUE),
                                  max=apply(rel_risks, 2, max, na.rm=TRUE))) %>%
                arrange(desc(max))
rownames(summary_df) <- colnames(rel_risks)
xkable(summary_df, caption='R^2 risk by algorithm')

rel_risks_long <- melt(rel_risks)
colnames(rel_risks_long) <- c('drug', 'learner', 'value')

# Excluding outlier gbmOOB for now (high peak at 0 messes up y limits)
ggplot(rel_risks_long %>% filter(learner != 'gbmOOB'), aes(x=value, fill=learner)) +
    geom_density(alpha=0.25) +
    geom_vline(xintercept=0, colour="red", linetype = "longdash") +
    theme(legend.justification=c(1,1), legend.position=c(1,1),
          legend.background=element_rect(fill=alpha('#333333', 0.4)),
          legend.text=element_text(size=10)) +
    xlim(-0.5, 0.5)
```

##### Number of relative risk scores > 0.10 for each learner

```{r, results='asis'}
interesting_hits <- as.data.frame(apply(rel_risks, 2, function (x) { sum(x > 0.10, na.rm=TRUE) }))
colnames(interesting_hits) <- 'num'
xkable(interesting_hits)
```

##### Number of NA's in CV risk output

```{r}
# learners
has_nas <- apply(cv_risks, 2, function(x) { sum(is.na(x)) })
has_nas[has_nas > 0]

# drugs
rownames(cv_risks)[apply(cv_risks, 1, function(x) { sum(is.na(x)) }) > 0]
```

#### Absolute vs. Relative risk

```{r, warning=FALSE}
abs_vs_rel <- cbind(cv_risks_long  %>% filter(learner != 'mean') %>%
                    dplyr::rename(absolute=value),
                    relative=rel_risks_long$value)

# Plot 4 at a time using facet grid
learners <- unique(abs_vs_rel$learner)
learner_groups <- split(learners, ceiling(seq_along(learners) / 4))

for (learners in learner_groups) {
    plt <- ggplot(abs_vs_rel %>% filter(learner %in% learners),
                  aes(x=relative, y=absolute, color=learner)) +
            geom_point(size=0.5) +
            geom_vline(xintercept=0, colour="#999999", linetype="longdash", size=0.3) +
            xlim(-0.5, 0.5) +
            facet_wrap(~learner, ncol=2) +
            theme(legend.position="none")
    print(plt)
}
```

### Feature importance

First, let's create a vector of feature types so that we can group our
plots this way.

```{r feature_types}
# types of predictor features
feature_types <- c('cnv_clusters', 'cnv_longest_gene', 'cnv_max_gene',
                   'cpdb_variants', 'cpdb_zscores', 'exome_variants',
                   'go_variants', 'go_zscores', 'msigdb_variants',
                   'msigdb_zscores', 'normed_counts', 'zscores_zscores')

ftype <- rep(NA, nrow(feature_importance))
for (cls in feature_types) {
    ftype[grepl(cls, rownames(feature_importance))] <- cls
}
```

#### Distribution of feature average variable importance

Median random forest (RF) variable importance for each feature across all drugs with a relative
CV risk of at least 0.10 for RF.

```{r}
# exclude low-performing drugs
drugs_to_include <- rownames(rel_risks)[rel_risks[,'randomForest'] >= 0.1]

feature_importance_filtered <- feature_importance[,colnames(feature_importance) %in%
                                                   drugs_to_include]
```

```{r variable_importance_dist, results='asis'}
median_feature_importance <- apply(feature_importance_filtered, 1, median)

# Create a feature/type mapping
median_type_importance <- as.data.frame(
    cbind(median_feature_importance=as.vector(median_feature_importance), type=ftype)
)

# Density grouped by feature type
ggplot(median_type_importance, aes(x=median_feature_importance,
                                   fill=type, color=type, group=type)) +
    geom_density(alpha=0.15) +
    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
          legend.background=element_rect(fill=alpha('#ffffff', 0)),
          legend.justification=c(1,1), legend.position=c(1,1)) +
    xlab('Median Random Forest Variable Importance') +
    guides(fill=guide_legend(ncol=2))

df <- median_type_importance %>%
        group_by(type) %>%
        summarize(avg=mean(as.numeric(median_feature_importance))) %>%
        arrange(desc(avg))
xkable(df)
```

#### Distribution of feature maximum variable importance

```{r variable_importance_dist2}
max_feature_importance <- apply(feature_importance_filtered, 1, max)

# Create a feature/type mapping
max_type_importance <- as.data.frame(
    cbind(max_feature_importance=as.vector(max_feature_importance), type=ftype)
)

# Density grouped by feature type
ggplot(max_type_importance, aes(x=max_feature_importance,
                                 fill=type, color=type, group=type)) +
    geom_density(alpha=0.15) +
    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
          legend.background=element_rect(fill=alpha('#ffffff', 0)),
          legend.justification=c(1,1), legend.position=c(1,1)) +
    xlab('Maximum Random Forest Variable Importance') +
    guides(fill=guide_legend(ncol=2))

df <- max_type_importance %>%
        group_by(type) %>%
        summarize(avg=mean(as.numeric(max_feature_importance))) %>%
        arrange(desc(avg))
xkable(df)
```

#### Features with highest average variable importance

Features ranked by _median_ random forest variable importance across all drugs.

```{r results='asis', message=FALSE}
highest_median_feature_importance <- as.data.frame(sort(median_feature_importance, decreasing=TRUE))
colnames(highest_median_feature_importance) <- c('Importance')
highest_median_feature_importance <-
    add_rownames(highest_median_feature_importance, 'Feature')

# add human-readable annotations
highest_median_feature_importance$Description = get_feature_descriptions(highest_median_feature_importance$Feature)

xkable(highest_median_feature_importance, 15,
       caption='Features ranked by median variable importance',
       str_max_width=60)

write.table(highest_median_feature_importance, sep='\t', quote=FALSE,
            file=file.path(output_dir, 'highest_median_feature_importance.tab'))
```

#### Features with highest maximum variable importance

Features ranked by _max_ random forest variable importance across all drugs.

```{r results='asis', message=FALSE}
highest_max_feature_importance <- as.data.frame(sort(max_feature_importance, decreasing=TRUE))
colnames(highest_max_feature_importance) <- c('Importance')
highest_max_feature_importance <- add_rownames(highest_max_feature_importance, 'Feature')

# add human-readable annotations
highest_max_feature_importance$Description = get_feature_descriptions(highest_max_feature_importance$Feature)

xkable(highest_max_feature_importance, 15,
       caption='Features ranked by max variable importance',
       str_max_width=60)

write.table(highest_max_feature_importance, sep='\t', quote=FALSE,
            file=file.path(output_dir, 'highest_max_feature_importance.tab'))
```

#### Features with the highest trimmed mean variable importance

Features ranked by _mean_ of their top N=10 variable importance scores. Since
different features are likely to help predict responses for different drugs,
this helps us to find features which performed well for at least some subset of
the drugs.

```{r results='asis', message=FALSE}
# Number of highest scoring drugs to average across
n <- 10

top_n <- apply(feature_importance_filtered, 1, function(x) { mean(head(sort(x, decreasing=TRUE), n)) })
trimmed_mean_import <- as.data.frame(sort(top_n, decreasing=TRUE))
colnames(trimmed_mean_import) <- c('Importance')
trimmed_mean_import <- add_rownames(trimmed_mean_import, 'Feature')

trimmed_mean_import$Description = get_feature_descriptions(trimmed_mean_import$Feature)

xkable(trimmed_mean_import, 15,
       caption='Features ranked by trimmed-mean variable importance.',
       str_max_width=60)

write.table(trimmed_mean_import, sep='\t', quote=FALSE,
            file=file.path(output_dir, 'trimmed_mean_importance.tab'))
```

### Drug performance

#### Drugs with shared predictive features

Next, we will see if there are any groups of drugs which are predicted by
similar sets of features. To do this, we will cluster the drugs based on the
their feature importance vectors.

```{r drug_heatmap, fig.width=1080/96, fig.height=1080/96, dpi=96}
heatmap_colors <- colorRampPalette(brewer.pal(9, "YlGnBu"))(100)
dist_matrix <- cor(feature_importance_filtered, method='spearman')

dendrogram <- ifelse(nrow(dist_matrix) > 100, 'none', 'column')

heatmap.2(dist_matrix, trace="none", labRow=NA, dendrogram=dendrogram,
          main="Drug prediction similarity", margins=c(12,8),
          xlab='Drug', ylab='Drug', col=heatmap_colors)
```

